---
layout: post
title:  "Training Google's Reformer and takeaways"
permalink: blog/reformer-99m/
tags: machine-learning training code google reformer

---


A lot of progress has been made in training large NLP models in recent years, and after [finetuning GPT2-1.5b](/blog/gpt-15b-chat-finetune/) I've been thinking about context window a lot. Large Transformers are amazing, but when you are only limited to 512 or more commonly now 1024 tokens, the model doesn't know quite that much of what happened before when generating text. Google's [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html) paper introduces a more efficent way of doing attention, which I used to train a model with context of 64k tokens. 


### Reformer


The two main tricks used by the Reformer are reversible residual layers and locality-sensitive-hashing (LSH). 

Reversible layers lowers memory consumption by allowing us to recalculate layer inputs during backprop based on the already computed later layers instead of keeping all of it in memory. It provides similar advantages to [memory checkpointing](https://github.com/cybertronai/gradient-checkpointing), except in this case the layers are designed specifically capture the layer-to-layer changes so you can do it more efficecntly.

More importantly, it also introduces LSH, which allows us to approximately compute attention much more cheaply and thus work with much, much larger context. In short LSH maps similar vectors together rather than working with all possible pairs of vectors (which is what makes attention so expensive). In the cases where we have many of the same/similar tokens, a lot of them will end up with the same hash so they get chunked and then attention is applied in parallel on those much smaller chunks.

If you are interested in the specific check out [the code](https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py), [paper](https://arxiv.org/abs/2001.04451) or [blog post](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html).


###

Google Research released an implementation of their Reformer model as part of the [trax library](https://github.com/google/trax), which is fairly easy to use and given my interest in NLP models with larger contexts I decided to experiment with it and pretrain the largest model you can fit on a Colab TPU - with the idea that this way others can take it and finetune it on their own tasks when large context is important.


I decided to train it on all of wikipedia, which I downloaded and preprocessed for making it easier and faster to use with Colab and Drive. I downloaded the most recent wikipedia dump and processed it into json while excluding all articles under 1000 characters - we are training for large contexts, so including tiny articles would be counter-productive. Then I split the big json file into smaller files of 5000 articles each - because Drive can lock down your file if it's too big and you access it too much. [Here](https://colab.research.google.com/drive/15WCdIYju3A4UOY1-JGt7NTlw0jMLJYXZ) is the Colab for it.

After that I used small amount of the data to play with Reformer, I started with their [code for training on Crime and Punishment](https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb), however, I just couldn't get it to go much beyond .3 accuracy no matter how much data I used. It seems like the parameters used there were mostly chosen for having a huge context but the modedl actually couldn't learn much. After a lot of experimenting I ended up with parameters closer to their [enwiki8 training run](#https://github.com/google/trax/blob/master/trax/configs/reformer_enwik8.gin
), I ran about 50 trials with small adjustments and chose the version of the parameters where the loss was going down quicker. You can see the final values I've chosen in the repo.

Next, I wanted to pre-tokenize all my data so I can load it to my model quick rather than doing it on the fly. One thing currently not added to the Reformer (but is on the list) is handling bigger vocab sizes, like the ~50k GPT2 vocabulary. Given that our context is going to be huge, and that the vocabulary size that enwiki8 config uses is 258 anyway, I went for a character-level ByteLevelBPE tokenizer from [huggingface's tokenizers](). I regreted it a bit, as there is little documentation in the library, and there are weird things in it like being unable to easily add extra tokens, or save the tokenizer - it took me a while to realize that saving after adding extra tokens does nothing, as they just save 'the model' as you've trained it, so I had to manually edit the vocabulary I pass to it. [Here]() is the Colab for tokenizing and saving all the data.